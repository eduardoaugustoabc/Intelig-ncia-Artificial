# -*- coding: utf-8 -*-
"""projetoFinalIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_EouwxW3ld1z9g5x2zDJiSwOMtZAoE0M

Trabalho apresentado como atividade avaliativa para compor a nota da disciplina de Inteligência Artificial do curso de graduaçao em Ciência da Computação da Universidade Federal de Campina Grande.

<br><br>
Proejto: Identificador de emoções em posts do X.
<br><br>
Alunos: Eduardo, João Pedro, João Victor, José Arthur e Rodrigo Rodrigues.

# Dependencias do projeto
"""

import nltk
nltk.download('stopwords')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from nltk.stem.snowball import SnowballStemmer
from keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import re
import nltk
from sklearn.metrics import confusion_matrix
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from google.colab import drive

"""# Importação dos dados"""

drive.mount("/content/drive")

df = pd.read_csv('/content/drive/MyDrive/UFCG/Trabalhos UFCG/IA/text.csv')
df.rename(columns={'text': 'Text', 'label': 'Label'}, inplace=True)
df.drop('Unnamed: 0',axis=1,inplace=True)

"""# Exibição dos dados

## Distribuição dos dados
"""

count = df['Label'].value_counts()
fig, axs = plt.subplots(1, 2, figsize=(12, 6), facecolor='white')

palette = sns.color_palette("viridis")
sns.set_palette(palette)
axs[0].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)
axs[0].set_title('Distribuição das categorias')

sns.barplot(x=count.index, y=count.values, ax=axs[1], palette="viridis")
axs[1].set_title('Contagem das categorias')

plt.tight_layout()

plt.show()

"""## Nuvem de palavras"""

# Criando dataset para visualização melhor dos dados
df_sadness = df[df['Label']==0]
df_joy = df[df['Label']==1]
df_love = df[df['Label']==2]
df_anger = df[df['Label']==3]
df_fear = df[df['Label']==4]
df_surprise = df[df['Label']==5]

combined_sadness_text = ' '.join(df_sadness['Text'])
combined_joy_text = ' '.join(df_joy['Text'])
combined_love_text = ' '.join(df_love['Text'])
combined_anger_text = ' '.join(df_anger['Text'])
combined_fear_text = ' '.join(df_fear['Text'])
combined_surprise_text = ' '.join(df_surprise['Text'])

#criação das núvens de palavras paras as palavras mais usadas em cada sentimento
sadness_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_sadness_text)
joy_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_joy_text)
love_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_love_text)
anger_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_anger_text)
fear_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_fear_text)
surprise_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_surprise_text)

plt.figure(figsize=(18, 9))

plt.subplot(2, 3, 1)
plt.imshow(sadness_wordcloud, interpolation='bilinear')
plt.title('Mensagens Tristes')
plt.axis('off')

plt.subplot(2, 3, 2)
plt.imshow(joy_wordcloud, interpolation='bilinear')
plt.title('Mensagens Alegres')
plt.axis('off')

plt.subplot(2, 3, 3)
plt.imshow(love_wordcloud, interpolation='bilinear')
plt.title('Mensagens de Amor')
plt.axis('off')

plt.subplot(2, 3, 4)
plt.imshow(anger_wordcloud, interpolation='bilinear')
plt.title('Mensagens Raivosas')
plt.axis('off')

plt.subplot(2, 3, 5)
plt.imshow(fear_wordcloud, interpolation='bilinear')
plt.title('Mensagens de medo')
plt.axis('off')

plt.subplot(2, 3, 6)
plt.imshow(surprise_wordcloud, interpolation='bilinear')
plt.title('Mensagens de Surpresa')
plt.axis('off')

plt.tight_layout()
plt.show()

"""# Pré-Processamento

Realizamos o pre processamento para um melhor funcionamento do modelo e para um treinamento mais eficiente, dessa forma o modelo iria focar unicamnte nas palavras presentes nos tweets, deixando de lado eleemntos que não são necessários e poderia causar uma mal interpretação ou mal funcionamento do modelos

Nesse processo  buscamos primeiro retirar possiveis links, tirando expressões de http, comuns em links para sistemas web. Nesse processo removemos caracteres não alfa numéricos, como parenteses, chaves e termos não alfanúmericos, outro ponto é reduzir o número de espaços, em casos de vários espaços seguidos buscamos reduzir para apenas um, além disso, removemos os digitos dos textos para que o modelo lide apenas com caracteres que representam letras

Após essas correções, também fazemos a remoção de stop words, que são palavras extremamente frequente, como conjunções e artigos, que iriam prejudicar completamente os termos mais frequentes, pois são comuns em qualquer sentimento
"""

#Processamento dos textos, retirando caracteres especiais, links, números e  stopwords
df['Text'] = df['Text'].str.replace(r'http\S+', '', regex=True)
df['Text'] = df['Text'].str.replace(r'[^\w\s]', '', regex=True)
df['Text'] = df['Text'].str.replace(r'\s+', ' ', regex=True)
df['Text'] = df['Text'].str.replace(r'\d+', '', regex=True)
df['Text'] = df['Text'].str.lower()
df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', x))

stop = stopwords.words('english')
df["Text"] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

"""# Separação dos dados"""

#Separação do dataset, sendo uma parte para o treinamento e uma para o teste
X = df['Text']
y = df['Label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Tokenização

A tokenização é o processo de dividir um texto em partes menores, chamadas tokens. Estes tokens geralmente são palavras individuais, mas também podem ser caracteres, subpalavras ou outras unidades textuais, dependendo do contexto e da necessidade da aplicação.

Primeiro criamos um Tokenizer para processar texto, limitando o vocabulário às 50.000 palavras mais frequentes. Após isso, ajustamos o Tokenizer ao texto tanto do conjunto de treinameto 'X_train' , como do conjunto de teste 'X_test, 'construindo um índice de palavras com base na frequência delas nesse conjunto. Depois, utilizamos o método texts_to_sequences() para converter as sequências de texto em sequências de tokens, onde cada texto no conjunto é substituído por uma sequências de números representando a posição da palavra correspondente no vocabulário. Por último, calculamos o comprimento máximo de uma sequência de tokens no conjunto de treinamento. Isso é útil para definir o comprimento máximo das sequências de entrada durante o pré-processamento, garantindo que todas as sequências tenham o mesmo comprimento ao alimentar o modelo de aprendizado.
"""

#tokenizando o dataset tanto de test como de treino
tokenizer = Tokenizer(num_words=50000)
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)
maxlen = max(len(tokens) for tokens in X_train_sequences)

"""# Construção do modelo

Aqui iremos construir o modelo, utilizando a arquitetura de redes neurais recorrentes bidirecionais (Bi-GRU). Começando com uma camada de incorporação (embedding) para representar palavras como vetores densos, o modelo continua com camadas de dropout para regularização e prevenir overfitting. Em seguida, são adicionadas duas camadas de Bi-GRU para capturar informações contextuais em ambas as direções do texto. Uma camada de normalização em lote é incluída para estabilizar e acelerar o treinamento. Por fim, uma camada densa com função de ativação softmax é empregada para classificar os textos em seis categorias de sentimentos. O modelo é compilado com o otimizador 'adam' e a função de perda 'sparse_categorical_crossentropy', enquanto a métrica de precisão (accuracy) é monitorada durante o treinamento. A exibição do resumo do modelo oferece uma visão geral de sua arquitetura e parâmetros.
"""

X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post',)
X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')
input_Size = np.max(X_train_padded) + 1

model = Sequential()

model.add(Embedding(input_dim=input_Size, output_dim=50, input_length=maxlen))

model.add(Dropout(0.5))

model.add(Bidirectional(GRU(120, return_sequences=True)))
model.add(Bidirectional(GRU(64, return_sequences=True)))

model.add(BatchNormalization())

model.add(Bidirectional(GRU(64)))

model.add(Dense(6, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

"""# Treinamento do modelo"""

history = model.fit(X_train_padded, y_train, epochs=5, batch_size=1500, validation_data=(X_test_padded, y_test))

"""# Validação e análise de métricas do modelo

Ao avaliar um modelo de machine learning, a validação desempenha um papel crucial. Dentre as métricas mais comuns, a acurácia se destaca por sua simplicidade: é a proporção de previsões corretas em relação ao total de exemplos.

Com o objetivo de análise mais detalhada do desempenho do modelo, é necessário considerar outras métricas, como a precisão e o recall. A precisão expressa a proporção de exemplos classificados como positivos que são verdadeiramente positivos, enquanto o recall indica a proporção de exemplos positivos verdadeiros que foram corretamente identificados pelo modelo. Em outras palavras, a precisão mede a qualidade das previsões positivas, enquanto o recall mede a capacidade do modelo de encontrar todos os exemplos positivos.

A matriz de confusão é uma ferramenta essencial para visualizar o desempenho de um modelo de classificação. Ela organiza as previsões do modelo em quatro categorias: verdadeiros positivos (TP), falsos positivos (FP), verdadeiros negativos (TN) e falsos negativos (FN). Com base nesses valores, é possível calcular métricas como a acurácia, precisão, recall e o F1-score.

O F1-score é a média harmônica entre precisão e recall, fornecendo uma única medida que leva em consideração tanto falsos positivos quanto falsos negativos. Essa métrica é particularmente útil quando há um desequilíbrio entre as classes.
"""

best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1

fig, axs = plt.subplots(1, 2, figsize=(16, 5))

axs[0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')
axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')
axs[0].scatter(best_epoch - 1, history.history['val_accuracy'][best_epoch - 1], color='green', label=f'Best Epoch: {best_epoch}')
axs[0].set_xlabel('Epoch')
axs[0].set_ylabel('Accuracy')
axs[0].set_title('Training and Validation Accuracy')
axs[0].legend()


axs[1].plot(history.history['loss'], label='Training Loss', color='blue')
axs[1].plot(history.history['val_loss'], label='Validation Loss', color='red')
axs[1].scatter(best_epoch - 1, history.history['val_loss'][best_epoch - 1], color='green',label=f'Best Epoch: {best_epoch}')
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('Loss')
axs[1].set_title('Training and Validation Loss')
axs[1].legend()

plt.tight_layout()
plt.show()

model.evaluate(X_test_padded, y_test)

y_pred = model.predict(X_test_padded)
y_pred = np.argmax(y_pred, axis=1)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')

# Calculo Precision, Recall e F1-score
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Exibir os resultados
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')

"""# Executando o modelo"""

model.save('model.h5')

model.load_weights('/content/drive/MyDrive/UFCG/Trabalhos UFCG/IA/model.h5')

def preprocess_message(message):
    message = re.sub(r'http\S+', '', message)  # Remover links
    message = re.sub(r'[^\w\s]', '', message)  # Remover caracteres especiais
    message = re.sub(r'\d+', '', message)  # Remover números
    message = message.lower()  # Converter para minúsculas
    stop = set(stopwords.words('english'))
    message = ' '.join([word for word in message.split() if word not in stop])  # Remover stopwords
    message = re.sub(r'[^a-zA-Z\s]', '', message)  # Remover caracteres não alfabéticos
    return message

def tokenize_and_pad(message):
    # Tokenizar a mensagem
    sequence = tokenizer.texts_to_sequences([message])
    # Preencher a sequência
    padded_sequence = pad_sequences(sequence, maxlen=maxlen, padding='post')
    return padded_sequence

# Preprocessamento da mensagem
def execute(message):

  preprocessed_message = preprocess_message(message)

  padded_message = tokenize_and_pad(preprocessed_message)

  prediction = model.predict(padded_message)

  predicted_class = np.argmax(prediction)

  if(predicted_class == 0):
    print('Mensagem triste')
  elif(predicted_class == 1):
    print('Mensagem feliz')
  elif(predicted_class == 2):
    print('Mensagem de amor')
  elif(predicted_class == 3):
    print('Mensagem raiva')
  elif(predicted_class == 4):
    print('Mensagem de medo')
  elif(predicted_class == 5):
    print('Mensagem de surpresa')

"""df_sadness = df[df['Label']==0]
df_joy = df[df['Label']==1]
df_love = df[df['Label']==2]
df_anger = df[df['Label']==3]
df_fear = df[df['Label']==4]
df_surprise = df[df['Label']==5]
"""

execute('I want to break free')

execute("I hate you")
